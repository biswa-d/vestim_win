{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecf2a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Standalone Model Testing Script\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook allows you to test a pre-trained PyTorch model (FNN, GRU, or LSTM) from the `vestim` project on a given dataset. \\n\",\n",
    "    \"\\n\",\n",
    "    \"**Instructions:**\\n\",\n",
    "    \"1. Fill in the parameters in the 'Configuration Parameters' cell below.\\n\",\n",
    "    \"2. Run the cells sequentially.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Imports\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"from torch.utils.data import DataLoader, TensorDataset\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import joblib # For loading scalers\\n\",\n",
    "    \"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from datetime import datetime\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Model Class Definitions\\n\",\n",
    "    \"\\n\",\n",
    "    \"These are the model definitions copied from the `vestim` project to ensure this notebook is self-contained. Make sure these match the definitions used during training.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class FNNModel(nn.Module):\\n\",\n",
    "    \"    def __init__(self, input_size, output_size, hidden_layer_sizes, dropout_prob=0.0):\\n\",\n",
    "    \"        super(FNNModel, self).__init__()\\n\",\n",
    "    \"        layers = []\\n\",\n",
    "    \"        current_dim = input_size\\n\",\n",
    "    \"        for hidden_dim in hidden_layer_sizes:\\n\",\n",
    "    \"            layers.append(nn.Linear(current_dim, hidden_dim))\\n\",\n",
    "    \"            layers.append(nn.ReLU())\\n\",\n",
    "    \"            if dropout_prob > 0:\\n\",\n",
    "    \"                layers.append(nn.Dropout(dropout_prob))\\n\",\n",
    "    \"            current_dim = hidden_dim\\n\",\n",
    "    \"        layers.append(nn.Linear(current_dim, output_size))\\n\",\n",
    "    \"        self.network = nn.Sequential(*layers)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        # For FNN, if input is (batch, seq_len, features), flatten seq_len*features\\n\",\n",
    "    \"        if x.ndim == 3:\\n\",\n",
    "    \"            x = x.view(x.size(0), -1)\\n\",\n",
    "    \"        return self.network(x)\\n\",\n",
    "    \"\\n\",\n",
    "    \"class GRUModel(nn.Module):\\n\",\n",
    "    \"    def __init__(self, input_size, hidden_units, num_layers, output_size=1, dropout_prob=0.0, device='cpu'):\\n\",\n",
    "    \"        super(GRUModel, self).__init__()\\n\",\n",
    "    \"        self.hidden_units = hidden_units\\n\",\n",
    "    \"        self.num_layers = num_layers\\n\",\n",
    "    \"        self.device = device\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.gru = nn.GRU(\\n\",\n",
    "    \"            input_size=input_size,\\n\",\n",
    "    \"            hidden_size=hidden_units,\\n\",\n",
    "    \"            num_layers=num_layers,\\n\",\n",
    "    \"            batch_first=True,\\n\",\n",
    "    \"            dropout=dropout_prob if num_layers > 1 else 0\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        self.fc = nn.Linear(hidden_units, output_size)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x, h_0=None):\\n\",\n",
    "    \"        if h_0 is None:\\n\",\n",
    "    \"            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_units).to(self.device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        out, _ = self.gru(x, h_0)\\n\",\n",
    "    \"        # Select the output of the last time step\\n\",\n",
    "    \"        out = self.fc(out[:, -1, :])\\n\",\n",
    "    \"        return out\\n\",\n",
    "    \"\\n\",\n",
    "    \"class LSTMModel(nn.Module):\\n\",\n",
    "    \"    def __init__(self, input_size, hidden_units, num_layers, device, output_size=1, dropout_prob=0.0):\\n\",\n",
    "    \"        super(LSTMModel, self).__init__()\\n\",\n",
    "    \"        self.hidden_units = hidden_units\\n\",\n",
    "    \"        self.num_layers = num_layers\\n\",\n",
    "    \"        self.device = device\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.lstm = nn.LSTM(\\n\",\n",
    "    \"            input_size=input_size,\\n\",\n",
    "    \"            hidden_size=hidden_units,\\n\",\n",
    "    \"            num_layers=num_layers,\\n\",\n",
    "    \"            batch_first=True,\\n\",\n",
    "    \"            dropout=dropout_prob if num_layers > 1 else 0\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        self.fc = nn.Linear(hidden_units, output_size)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x, h_s=None, h_c=None):\\n\",\n",
    "    \"        if h_s is None or h_c is None:\\n\",\n",
    "    \"            h_s = torch.zeros(self.num_layers, x.size(0), self.hidden_units).to(self.device)\\n\",\n",
    "    \"            h_c = torch.zeros(self.num_layers, x.size(0), self.hidden_units).to(self.device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        out, _ = self.lstm(x, (h_s, h_c))\\n\",\n",
    "    \"        # Select the output of the last time step\\n\",\n",
    "    \"        out = self.fc(out[:, -1, :])\\n\",\n",
    "    \"        return out\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Configuration Parameters\\n\",\n",
    "    \"\\n\",\n",
    "    \"**IMPORTANT:** Update these parameters with your specific paths, column names, and model architecture details.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# --- Configuration Parameters ---\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Data Parameters\\n\",\n",
    "    \"TEST_DATA_PATH = \\\"path/to/your/test_data.csv\\\"  # Path to your single test CSV file\\n\",\n",
    "    \"MODEL_PATH = \\\"path/to/your/model.pt\\\"          # Path to your saved .pt or .pth model file\\n\",\n",
    "    \"SCALER_TARGET_PATH = None # e.g., \\\"path/to/your/scaler_target.joblib\\\" or None if no target scaling\\n\",\n",
    "    \"SCALER_FEATURES_PATH = None # e.g., \\\"path/to/your/scaler_features.joblib\\\" or None if no feature scaling\\n\",\n",
    "    \"OUTPUT_DIR = \\\".\\\" # Directory to save predictions CSV and plot (default: current directory)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature and Target Columns\\n\",\n",
    "    \"FEATURE_COLUMNS = ['feature1', 'feature2', 'feature3'] # List of feature column names used for training\\n\",\n",
    "    \"TARGET_COLUMN = 'target_value'                       # Target column name used for training\\n\",\n",
    "    \"\\n\",\n",
    "    \"# If features were scaled, provide the list of columns that were scaled (must match scaler_features)\\n\",\n",
    "    \"# This is important for inverse transforming if SCALER_FEATURES_PATH is provided.\\n\",\n",
    "    \"FEATURE_COLUMNS_TO_SCALE = None # e.g., ['feature1', 'feature2'] or None. \\n\",\n",
    "    \"                                # If None and SCALER_FEATURES_PATH is set, it will try to use all FEATURE_COLUMNS.\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Model Parameters\\n\",\n",
    "    \"MODEL_TYPE = 'LSTM' # Choose from 'LSTM', 'GRU', or 'FNN'\\n\",\n",
    "    \"LOOKBACK = 20       # Sequence length / lookback window used during training\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Common Model Architecture\\n\",\n",
    "    \"# input_dim will be derived from len(FEATURE_COLUMNS)\\n\",\n",
    "    \"OUTPUT_DIM = 1      # Typically 1 for regression tasks\\n\",\n",
    "    \"\\n\",\n",
    "    \"# FNN Specific Architecture (only fill if MODEL_TYPE is 'FNN')\\n\",\n",
    "    \"FNN_HIDDEN_DIMS = [128, 64] # List of hidden layer sizes for FNN\\n\",\n",
    "    \"FNN_DROPOUT_RATE = 0.0      # Dropout rate for FNN\\n\",\n",
    "    \"\\n\",\n",
    "    \"# RNN (LSTM/GRU) Specific Architecture (only fill if MODEL_TYPE is 'LSTM' or 'GRU')\\n\",\n",
    "    \"RNN_HIDDEN_UNITS = 64\\n\",\n",
    "    \"RNN_NUM_LAYERS = 2\\n\",\n",
    "    \"RNN_DROPOUT_PROB = 0.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"# DataLoader Parameters\\n\",\n",
    "    \"BATCH_SIZE = 32     # Batch size for testing (can be 1 for sequential processing)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Device Configuration\\n\",\n",
    "    \"DEVICE = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Using device: {DEVICE}\\\")\\n\",\n",
    "    \"# --- End of Configuration ---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Helper Functions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def create_sequences(data_df, feature_cols, target_col, lookback):\\n\",\n",
    "    \"    \\\"\\\"\\\"Creates sequences from a DataFrame.\\\"\\\"\\\"\\n\",\n",
    "    \"    X_list, y_list = [], []\\n\",\n",
    "    \"    df_features = data_df[feature_cols].values\\n\",\n",
    "    \"    df_target = data_df[target_col].values\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for i in range(len(data_df) - lookback):\\n\",\n",
    "    \"        X_list.append(df_features[i:(i + lookback), :])\\n\",\n",
    "    \"        y_list.append(df_target[i + lookback])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return np.array(X_list), np.array(y_list)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_test_data(file_path, feature_cols, target_col, lookback, batch_size):\\n\",\n",
    "    \"    \\\"\\\"\\\"Loads data, creates sequences, and returns a DataLoader.\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        df = pd.read_csv(file_path)\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        print(f\\\"Error: Test data file not found at {file_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error reading test data file: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Ensure all feature and target columns exist\\n\",\n",
    "    \"    missing_cols = [col for col in feature_cols + [target_col] if col not in df.columns]\\n\",\n",
    "    \"    if missing_cols:\\n\",\n",
    "    \"        print(f\\\"Error: Missing columns in test data: {missing_cols}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    X_np, y_np = create_sequences(df, feature_cols, target_col, lookback)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if X_np.size == 0 or y_np.size == 0:\\n\",\n",
    "    \"        print(f\\\"Error: No sequences created. Check lookback ({lookback}) vs data length ({len(df)}).\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    X_tensor = torch.tensor(X_np, dtype=torch.float32)\\n\",\n",
    "    \"    y_tensor = torch.tensor(y_np, dtype=torch.float32).unsqueeze(1) # Ensure y is [samples, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    dataset = TensorDataset(X_tensor, y_tensor)\\n\",\n",
    "    \"    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\\n\",\n",
    "    \"    return data_loader\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_model_from_path(model_type, model_path, input_dim, output_dim, device, \\n\",\n",
    "    \"                           fnn_hidden_dims, fnn_dropout, \\n\",\n",
    "    \"                           rnn_hidden_units, rnn_num_layers, rnn_dropout_prob, lookback_for_fnn_input):\\n\",\n",
    "    \"    \\\"\\\"\\\"Loads the model based on type and parameters.\\\"\\\"\\\"\\n\",\n",
    "    \"    if model_type == 'FNN':\\n\",\n",
    "    \"        # For FNN, input_dim is features * lookback if sequence is flattened\\n\",\n",
    "    \"        # Assuming the FNNModel handles flattening if x.ndim == 3, \\n\",\n",
    "    \"        # or expects flattened input if x.ndim == 2.\\n\",\n",
    "    \"        # The create_sequences function prepares X as (samples, lookback, features)\\n\",\n",
    "    \"        # So, FNNModel's forward will flatten it. input_size for FNN should be num_features * lookback\\n\",\n",
    "    \"        model_input_dim = input_dim * lookback_for_fnn_input\\n\",\n",
    "    \"        model = FNNModel(input_size=model_input_dim, \\n\",\n",
    "    \"                         output_size=output_dim, \\n\",\n",
    "    \"                         hidden_layer_sizes=fnn_hidden_dims, \\n\",\n",
    "    \"                         dropout_prob=fnn_dropout)\\n\",\n",
    "    \"    elif model_type == 'LSTM':\\n\",\n",
    "    \"        model = LSTMModel(input_size=input_dim, \\n\",\n",
    "    \"                          hidden_units=rnn_hidden_units, \\n\",\n",
    "    \"                          num_layers=rnn_num_layers, \\n\",\n",
    "    \"                          output_size=output_dim,\\n\",\n",
    "    \"                          dropout_prob=rnn_dropout_prob, \\n\",\n",
    "    \"                          device=device)\\n\",\n",
    "    \"    elif model_type == 'GRU':\\n\",\n",
    "    \"        model = GRUModel(input_size=input_dim, \\n\",\n",
    "    \"                         hidden_units=rnn_hidden_units, \\n\",\n",
    "    \"                         num_layers=rnn_num_layers, \\n\",\n",
    "    \"                         output_size=output_dim,\\n\",\n",
    "    \"                         dropout_prob=rnn_dropout_prob, \\n\",\n",
    "    \"                         device=device)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Error: Unknown model type '{model_type}'\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        model.load_state_dict(torch.load(model_path, map_location=device))\\n\",\n",
    "    \"        print(f\\\"Model loaded successfully from {model_path}\\\")\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        print(f\\\"Error: Model file not found at {model_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading model state_dict: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    model.to(device)\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    return model\\n\",\n",
    "    \"\\n\",\n",
    "    \"def calculate_mape(y_true, y_pred):\\n\",\n",
    "    \"    y_true, y_pred = np.array(y_true), np.array(y_pred)\\n\",\n",
    "    \"    non_zero_true = y_true != 0\\n\",\n",
    "    \"    if not np.any(non_zero_true):\\n\",\n",
    "    \"        return np.nan # Avoid division by zero if all true values are zero\\n\",\n",
    "    \"    return np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Main Testing Logic\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# 1. Load Data\\n\",\n",
    "    \"print(\\\"Loading test data...\\\")\\n\",\n",
    "    \"test_loader = load_test_data(TEST_DATA_PATH, FEATURE_COLUMNS, TARGET_COLUMN, LOOKBACK, BATCH_SIZE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if test_loader is None:\\n\",\n",
    "    \"    print(\\\"Halting execution due to data loading error.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Test data loaded successfully.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 2. Load Model\\n\",\n",
    "    \"    print(\\\"\\\\nLoading model...\\\")\\n\",\n",
    "    \"    input_dim = len(FEATURE_COLUMNS)\\n\",\n",
    "    \"    model = load_model_from_path(MODEL_TYPE, MODEL_PATH, input_dim, OUTPUT_DIM, DEVICE,\\n\",\n",
    "    \"                                 FNN_HIDDEN_DIMS, FNN_DROPOUT_RATE,\\n\",\n",
    "    \"                                 RNN_HIDDEN_UNITS, RNN_NUM_LAYERS, RNN_DROPOUT_PROB, LOOKBACK)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if model is None:\\n\",\n",
    "    \"        print(\\\"Halting execution due to model loading error.\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # 3. Load Scalers (if provided)\\n\",\n",
    "    \"        scaler_target = None\\n\",\n",
    "    \"        scaler_features = None\\n\",\n",
    "    \"        if SCALER_TARGET_PATH:\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                scaler_target = joblib.load(SCALER_TARGET_PATH)\\n\",\n",
    "    \"                print(f\\\"Target scaler loaded from {SCALER_TARGET_PATH}\\\")\\n\",\n",
    "    \"            except Exception as e:\\n\",\n",
    "    \"                print(f\\\"Warning: Could not load target scaler from {SCALER_TARGET_PATH}: {e}\\\")\\n\",\n",
    "    \"        if SCALER_FEATURES_PATH:\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                scaler_features = joblib.load(SCALER_FEATURES_PATH)\\n\",\n",
    "    \"                print(f\\\"Features scaler loaded from {SCALER_FEATURES_PATH}\\\")\\n\",\n",
    "    \"                if FEATURE_COLUMNS_TO_SCALE is None:\\n\",\n",
    "    \"                    print(f\\\"FEATURE_COLUMNS_TO_SCALE is None, assuming all FEATURE_COLUMNS were scaled by scaler_features.\\\")\\n\",\n",
    "    \"                    _feature_cols_for_scaling = FEATURE_COLUMNS\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    _feature_cols_for_scaling = FEATURE_COLUMNS_TO_SCALE\\n\",\n",
    "    \"            except Exception as e:\\n\",\n",
    "    \"                print(f\\\"Warning: Could not load features scaler from {SCALER_FEATURES_PATH}: {e}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 4. Perform Inference\\n\",\n",
    "    \"        print(\\\"\\\\nPerforming inference...\\\")\\n\",\n",
    "    \"        all_predictions = []\\n\",\n",
    "    \"        all_actuals = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            for X_batch, y_batch in test_loader:\\n\",\n",
    "    \"                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if MODEL_TYPE == 'FNN':\\n\",\n",
    "    \"                    # FNNModel's forward handles flattening if X_batch is (batch, seq, features)\\n\",\n",
    "    \"                    predictions = model(X_batch)\\n\",\n",
    "    \"                elif MODEL_TYPE in ['LSTM', 'GRU']:\\n\",\n",
    "    \"                    # RNNs expect (batch, seq_len, features)\\n\",\n",
    "    \"                    predictions = model(X_batch) \\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    print(f\\\"Model type {MODEL_TYPE} not handled in inference loop.\\\")\\n\",\n",
    "    \"                    break\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                all_predictions.extend(predictions.cpu().numpy())\\n\",\n",
    "    \"                all_actuals.extend(y_batch.cpu().numpy())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        all_predictions = np.array(all_predictions).flatten()\\n\",\n",
    "    \"        all_actuals = np.array(all_actuals).flatten()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # 5. Inverse Transform (if scalers were used)\\n\",\n",
    "    \"        predictions_final = all_predictions.copy()\\n\",\n",
    "    \"        actuals_final = all_actuals.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if scaler_target:\\n\",\n",
    "    \"            print(\\\"\\\\nInverse transforming predictions and actuals using target scaler...\\\")\\n\",\n",
    "    \"            # Scaler expects 2D array [n_samples, n_features=1]\\n\",\n",
    "    \"            predictions_final = scaler_target.inverse_transform(predictions_final.reshape(-1, 1)).flatten()\\n\",\n",
    "    \"            actuals_final = scaler_target.inverse_transform(actuals_final.reshape(-1, 1)).flatten()\\n\",\n",
    "    \"            # Note: If features were also scaled and influenced the target scale (e.g. diffs),\\n\",\n",
    "    \"            # this simple inverse transform might not be enough. This assumes target was scaled independently.\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # 6. Calculate Metrics\\n\",\n",
    "    \"        print(\\\"\\\\nCalculating metrics...\\\")\\n\",\n",
    "    \"        mae = mean_absolute_error(actuals_final, predictions_final)\\n\",\n",
    "    \"        mse = mean_squared_error(actuals_final, predictions_final)\\n\",\n",
    "    \"        rmse = np.sqrt(mse)\\n\",\n",
    "    \"        r2 = r2_score(actuals_final, predictions_final)\\n\",\n",
    "    \"        mape = calculate_mape(actuals_final, predictions_final)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        print(f\\\"\\\\n--- Test Metrics ---\\\")\\n\",\n",
    "    \"        print(f\\\"MAE:  {mae:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"MSE:  {mse:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"RMSE: {rmse:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"RÂ²:   {r2:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"MAPE: {mape:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # 7. Save Predictions\\n\",\n",
    "    \"        print(\\\"\\\\nSaving predictions...\\\")\\n\",\n",
    "    \"        timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n\",\n",
    "    \"        model_name_part = os.path.splitext(os.path.basename(MODEL_PATH))[0]\\n\",\n",
    "    \"        predictions_df = pd.DataFrame({\\n\",\n",
    "    \"            'Actual': actuals_final,\\n\",\n",
    "    \"            'Predicted': predictions_final\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"        predictions_filename = os.path.join(OUTPUT_DIR, f\\\"predictions_{model_name_part}_{timestamp}.csv\\\")\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            predictions_df.to_csv(predictions_filename, index=False)\\n\",\n",
    "    \"            print(f\\\"Predictions saved to {predictions_filename}\\\")\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error saving predictions: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # 8. Plot Results (optional)\\n\",\n",
    "    \"        print(\\\"\\\\nPlotting results...\\\")\\n\",\n",
    "    \"        plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"        plt.plot(actuals_final, label='Actual Values', color='blue', alpha=0.7)\\n\",\n",
    "    \"        plt.plot(predictions_final, label='Predicted Values', color='red', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        plt.title(f'Actual vs. Predicted Values ({model_name_part})')\\n\",\n",
    "    \"        plt.xlabel('Sample Index')\\n\",\n",
    "    \"        plt.ylabel('Target Value')\\n\",\n",
    "    \"        plt.legend()\\n\",\n",
    "    \"        plt.grid(True)\\n\",\n",
    "    \"        plot_filename = os.path.join(OUTPUT_DIR, f\\\"plot_{model_name_part}_{timestamp}.png\\\")\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            plt.savefig(plot_filename)\\n\",\n",
    "    \"            print(f\\\"Plot saved to {plot_filename}\\\")\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error saving plot: {e}\\\")\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.x.x\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
